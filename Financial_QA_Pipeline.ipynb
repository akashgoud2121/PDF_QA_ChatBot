{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ab87f3",
   "metadata": {},
   "source": [
    "\n",
    "# Retrieval-Augmented Generation (RAG) Model for QA Bot on P&L Data\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) pipeline using LangChain, FAISS, and Google Generative AI. \n",
    "The process involves extracting text from PDFs, creating a vector store, and performing QA (Question Answering) with generative AI. \n",
    "Additionally, we calculate the accuracy of the responses by comparing them against ground truth data.\n",
    "\n",
    "\n",
    "The required dependencies are installed at the beginning,Ensure all dependencies are installed before running the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938105c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Required installations\n",
    "%pip install os\n",
    "%pip install langchain\n",
    "%pip install langchain_google_genai\n",
    "%pip install PyPDF2\n",
    "%pip install faiss-cpu\n",
    "%pip install langchain_community\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f9fc6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Import Necessary Libraries\n",
    "This step imports all the required libraries, including:\n",
    "- `os` for managing environment variables and file paths.\n",
    "- `PyPDF2` for extracting text from PDF files.\n",
    "- `LangChain` modules for text processing, embeddings, vector storage, and question-answering.\n",
    "- `FAISS` for efficient similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0aa90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\Project_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c92de1",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Define Helper Functions\n",
    "We define helper functions to:\n",
    "1. Extract text from a PDF file.\n",
    "2. Split the text into smaller chunks for easier processing and embedding.\n",
    "3. Create a FAISS vector store for similarity search, storing the index in a specified folder.\n",
    "4. Create the RAG model for question-answering using Google Generative AI and LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file using PyPDF2.\n",
    "    \"\"\"\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bfcab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_text_chunks(text):\n",
    "    \"\"\"\n",
    "    Splits the loaded text into chunks for embedding and retrieval.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    return text_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4deedfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_vector_store(text_chunks, index_path=\"faiss_index\"):\n",
    "    \"\"\"\n",
    "    Embeds the text chunks into a vector store for similarity search.\n",
    "    Saves the FAISS index to the specified folder.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(index_path):\n",
    "        os.makedirs(index_path)\n",
    "\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(index_path)\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "298cea06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rag_model():\n",
    "    \"\"\"\n",
    "    Creates a conversational chain for QA using LangChain and Google Generative AI.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context. Make sure to provide all the details. \n",
    "    If the answer is not in the provided context, just say, \\\"Answer is not available in the context.\\\" Don't provide a wrong answer.\n",
    "\n",
    "    Context:\\n{context}\\n\n",
    "    Question:\\n{question}\\n\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d2523",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Define Input Variables\n",
    "In this step, we set up the input variables:\n",
    "- `api_key`: The Google API key to access Generative AI services.To get API key visit https://aistudio.google.com/app/apikey and Create your own api key.\n",
    "- `pdf_path`: The path to the PDF file.\n",
    "- `user_question`: An example question to test the RAG pipeline.\n",
    "- `index_folder`: The folder where FAISS index files will be stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "72a4f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "api_key = \"Enter Your own API key here\"  # Replace with your actual API key\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
    "\n",
    "pdf_path = (r\"\\Sample Financial Statement.pdf\")  # Replace with the path to your PDF file\n",
    "user_question = \"What is the gross profit for Q3 2024?\"  # Example question\n",
    "index_folder = \"faiss_index\"  # Folder to save the FAISS index\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268b0afc",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Process the PDF and Run the QA Pipeline\n",
    "This step involves:\n",
    "1. Extracting text from the PDF.\n",
    "2. Splitting the text into chunks.\n",
    "3. Creating a FAISS vector store.\n",
    "4. Performing similarity search on the vector store.\n",
    "5. Running the RAG model to generate an answer to the question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dd3d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "    11,175\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    # Extract text from the PDF\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # Split text into chunks\n",
    "    text_chunks = get_text_chunks(raw_text)\n",
    "\n",
    "    # Create the vector store and save it to the folder\n",
    "    vector_store = get_vector_store(text_chunks, index_path=index_folder)\n",
    "\n",
    "    # Perform similarity search\n",
    "    docs = vector_store.similarity_search(user_question)\n",
    "\n",
    "    # Initialize the RAG model\n",
    "    chain = get_rag_model()\n",
    "\n",
    "    # Get the response\n",
    "    response = chain({\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True)\n",
    "\n",
    "    # Display the response\n",
    "    print(\"Response:\")\n",
    "    print(response.get(\"output_text\", \"No response generated.\"))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81118541",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Evaluate the Model's Accuracy\n",
    "We calculate the accuracy of the RAG model by comparing the generated responses with the expected answers.\n",
    "This step assumes that a `ground_truth` dictionary is available containing questions and their expected answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e25a8c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "Question: What is the gross profit for Q3 2024?\n",
      "Predicted Answer: 11,175\n",
      "True Answer: 11,175\n",
      "--------------------------------------------------\n",
      "Question: What is the net revenue for the year 2023?\n",
      "Predicted Answer: 146,767\n",
      "True Answer: 146,767\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accuracy Calculation\n",
    "# Define a ground truth dataset as a dictionary of questions and their expected answers\n",
    "ground_truth = {\n",
    "    \"What is the gross profit for Q3 2024?\": \"11,175\",\n",
    "    \"What is the net revenue for the year 2023?\": \"146,767\",\n",
    "    \n",
    "    \n",
    "    # Add more questions and their expected answers here\n",
    "}\n",
    "\n",
    "# Generate predictions using the pipeline\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for question, true_answer in ground_truth.items():\n",
    "    docs = vector_store.similarity_search(question)  # Perform similarity search\n",
    "    response = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)\n",
    "    predicted_answer = response.get(\"output_text\", \"No response generated.\")\n",
    "    \n",
    "    predictions.append(predicted_answer.strip())\n",
    "    true_labels.append(true_answer.strip())\n",
    "\n",
    "# Calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "try:\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(\"Error in calculating accuracy:\", e)\n",
    "\n",
    "# Optionally, display predictions and true answers for inspection\n",
    "for i, question in enumerate(ground_truth.keys()):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Predicted Answer: {predictions[i]}\")\n",
    "    print(f\"True Answer: {true_labels[i]}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1476dd2",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "This notebook demonstrated the RAG pipeline with FAISS and Google Generative AI. \n",
    "We successfully processed a PDF, created a FAISS index, performed similarity search, generated responses, and evaluated the model's accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
